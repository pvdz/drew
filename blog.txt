Introducing Drew

Drew, the Declarative Rewriting Expression Wengine, is a tool you can use to do simple code detections or transformations. I've often said that most often you need a stream of tokens more than you need a parse tree. Well this tool puts that to the test.

I've started writing the tool in March or something, while refactoring code for a client. I kept doing the same things over and over again, and couldn't really see a simple tool to do this job for me. Yes there are beautifiers, but no they don't apply the wonky style guide I needed.

I started seeing patterns and figured I may as well codify those patterns into a tool. Because tools. Hence Drew was born.

To use Drew you need some kind of preprocessor, most likely a lexer of sorts, to chunk your input into "tokens". In this context a token is a simple object with specific structure (or api or whatever you want to call it) which contains data on the value, the type of the token, and the number of the token. Although I may actually not even use the token index, I'm not sure (need to check).

It's also possible to use Drew on plain text; each character will simply become a token and the token type for all of them become some generic non-white-space type.

Using this information you can write queries in a a custom language (DSL) which resembles regular expressions. Drew then applies that query much like <code>string.replace(/foo/, function(){})</code> does and calls your callback whenever a match is found. Or more often if you desire.

Since it's [i]like[/i] regular expressions it can do certain stuff better and other stuff worse than regular regular expressions. For example Drew can't do non-greedy matches but you can trigger early callbacks for partial matches and move the token pointer on demand. Ok actually Drew can do a lot more than regular expressions but the basic gist is the same; recursive descent pattern matching.

So what's this language look like? Well there's an extensive documentation that covers all the features. But let's cover some basics.

Since Drew works on tokens, we're going to match tokens. We identify two different tokens, this is mainly relevant for source code. There are black tokens and white tokens. Black tokens are only those tokens which are not considered "white space". White space includes of course the spaces, the newlines, but also the comments and in the case of JS it includes the ASI tokens. White tokens don't make this distinction; any token is considered white token. So some tokens are only white while others are black and white.

This black and white distinction is relevant because often you'll want to know whether the variable name was preceded by a keyword or not. You don't want to have to figure out whether there were any spaces or newlines or anything else in between; it's most often simply irrelevant to you. 

With Drew you select on individual token values. This means you don't match on something like <code>/var\s+foo\s+;?</code>, but instead you match token by token and have the ability to ignore whitespace cases: <code>{`var`}{`foo`}{`;`}?</code>. This example means something like "find the first black token, check if its value is literally <code>var</code>, find the next black token, check if its value is literally <code>foo</code>, find the next black token, check if its value is literally <code>;</code> but if there is no such token next then that's okay too. The question mark works the same as regular expressions; the match is optional. [i]This example isn't perfectly generic it's only meant to demonstrate the syntax[/i]

If we wanted to find all variables instead of explicitly <code>foo</code> we could change the query to <code>{`var`}{IDENTIFIER}{`;`}?</code>. The <code>IDENTIFIER</code> there is not back-ticked so it will resolve to a constant or macro. In this case a constant which does <code>type(IDENTIFIER)</code>, something specific for ZeParser. Constants are arbitrary JS code which are compiled into the query function.

That's right; compiled. Your query is parsed and a new function is generated. This function does the actual parsing using some boiler plate functions to help in traversing the token stream back and forward. Basic compilation consists of checking each token condition and if it matches, check the next token condition, etc. In case of a quantifier the check happens multiple times and a condition flag is updated every time until it fails or the max or EOF has been reached. 

Quantifiers can be a numbered range or special symbols taken from regular expressions. So you can be <code>[x]?</code>, <code>[x]*</code>, or <code>[x]+</code>. These have the same meaning as in regular expressions (zero or 1, 0 or more, 1 or more). You can also use an explicit count with <code>[x]5</code>, or an explicit range with <code>[x]3...5</code>, or a lower or upper bound only by omitting the left or right number <code>[x]4...</code> and <code>[x]...4</code>. I've chosen for a slightly different syntax opposed to regular expressions because I think the triple dots are more obvious in terms of meaning "range", and because the curly brackets were already used for token boundaries so it would become ambiguous to use them.

After the quantifier you can do something special, compared to regular expressions. You can assign the last atom result (the previous token or group, depending) explicitly to the callback function args. You can choose to map them directly to arguments by index (<code>[x]=0[y]=1</code>) or give them an explicit name (<code>[x]=first [y]=second</code>) so the callback receives an object with each name as a key instead. If you only use numbers as names and don't explicitly override the <code>0</code>, it will implicitly be the start of the match. I think this is much better than how regular expressions work with their implicit anonymous groupings.

These names, I dubbed them "designators", retain the last value before a match was found. But only if the token indeed is part of the match. Backtracking properly takes this into account. Overriding is not a problem because you can trigger an explicit callback invocation mid-way a match with <code>#</code>. The hash will queue a callback and if that part of the query becomes part of the final match, and the whole query finds a match, the callback will be invoked multiple times. <code>[x]=0([y]=1 #)*</code> calls the callback with the same <code>x</code> but different <code>y</code> tokens, one for each found. Or only one if no <code>y</code> token was found at all.

Lastly the (optional) quantifier may be followed by a comment which is started with a colon (<code>:</code>). The comment can contain anything except <code>[{</code> because that'd be ambiguous. This allows you to add inline comments to clarify arguments or parts of the match. On a similar note; all whitespace that's not inside a literal is ignored and therefor insignificant. Whitespace is spaces, tabs, and newlines (<code>\r</code> and <code>\n</code>). You can add them nearly much anywhere. This was easy to do since all matches have token boundaries and so the top level doesn't have literal stuff.

Ah and grouping. Parenthesis allow grouping. They are non-capturing but can have the same quantifier and designator suffix as tokens do. So if you wanted, a group can be capturing, but in Drew it's an opt-in system opposed to regular expressions where that is opt-out.

Matching token values may not suffice in some cases. However you can also do partial matches with regular expressions: <code>{IDENTIFIER & /^get/}</code>, this matches on any identifier token whose value starts with <code>get</code>. Underwater this is a simple <code>regex.test(str)</code> so you must add the <code^</code> and <code>$</code> yourself if this is relevant. Otherwise a partial match also passes. The only flag that's supported (or even valid) is <code>i</code>, which means "case insensitive match". The same flag is valid for literals btw, <code>[`tostring`i]</code> to have Drew do a <code>.toLowerCase()</code> to both sides before matching the literal with the token value. More expensive, obviously but if you need it...

The <code>^</code> and <code>$</code> symbols work slightly different from though similar to regular expressions. <code>^</code> matches if the previous white token is a newline. Similarly <code>$</code> matches if the current white token is a newline. To aid these symbols I've coded <code>~</code> to skip whitespace except for newlines. These are toplevel symbols so not to be used inside a token wrapper. To match the start and end of the whole input, regardless of current position, you can use the <code>^^</code> and <code>$$</code> symbols. The others will also match there but the difference is that the latter won't match on actual newlines.

Another such symbol are <code>&lt;</code>, <code>&lt;&lt;</code>, <code>&gt;</code>, and <code>&gt;&gt;</code>. They allow you to skip one white or black token forward or backwards, unconditionally. To apply the operator multiple times you can append a positive integer to the op. This op is whitespace sensitive (for disambiguation), meaning that <code>> ></code> is different from <code>>></code>. This is a powerful tool that allows you to pull some pointer seeking tricks and optimizations that would otherwise not be possible. Like a quick token check first, then matching stuff, and only if that matches go back to the first match to do some further validation. It also allows you to use the <code>AFTER</code> repeat mode while still matching the last token of the previous match. Though these are only for the more complex cases. 

You can use <code>&amp;</code> and <code>|</code> in and between tokens to extend a condition to require the left AND right, or left OR right respectfully. Obviously a <code>&amp;</code> between tokens is meaningless as this is implicitly always the case (<code>[x]&[y]</code> is always the case with <code>[x][y]</code>). It's useful inside tokens though, like with ZeParser because it turns comments into white tokens, so to find tokens you have to do <code>[WHITE &amp; /^\//]</code>; find tokens of type=white that start with a forward slash. The precedence rules are simple; left to right. No priority. Binds to an atom left and right (so a single token or a group).

Another condition that should not be a problem to understand is the <code>!</code>, which inverts the matching state of the first next atom (token or group); <code>[x]![y]</code>. This can be used inside and outside of tokens.

Drew also offers a macro and constant system, which could be viewed as a plugin system. When parsing the query Drew will extrapolate macros to their individual parts, recursively. So you can define macros using other macros or constants. When generating the query function code, constants are replaced by their code as-is. This code should be an expression, not statement. When in doubt, your constant value should always be "valid" inside a log, like <code>console.log(CONSTANT);</code>. No constants are not put inside log statements, but this hopefully clarifies the conditions to which constants should adhere :)

// supply a function to fetch tokens?
